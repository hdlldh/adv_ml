{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow (3 pts)\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb.\r\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    %tensorflow_version 1.x\n",
    "    \n",
    "    if not os.path.exists('.setup_complete'):\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "        !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffba0f6e3d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input variables. We only need <s, a, r> for REINFORCE\n",
    "ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n",
    "ph_actions = tf.placeholder('int32', name=\"action_ids\")\n",
    "ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "network = Sequential()\n",
    "network.add(Dense(256, input_shape=state_dim, activation='relu'))\n",
    "network.add(Dense(128, activation='relu'))\n",
    "network.add(Dense(n_actions))\n",
    "\n",
    "logits = network(ph_states)\n",
    "\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    return policy.eval({ph_states: [states]})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(s)\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice((0, 1), p=action_probs)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    G = [0] * len(rewards)\n",
    "    for i in range(len(rewards)-2, -1, -1):\n",
    "        G[i] = gamma*G[i+1] + rewards[i]\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n",
    "# You may use log_policy_for_actions to get log probabilities for actions taken.\n",
    "# Also recall that we defined ph_cumulative_rewards earlier.\n",
    "\n",
    "J = tf.reduce_mean(log_policy_for_actions*ph_cumulative_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n",
    "\n",
    "$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n",
    "# being deterministic, harming exploration.\n",
    "\n",
    "entropy = -tf.reduce_sum(policy*log_policy,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Maximizing X is the same as minimizing -X, hence the sign.\n",
    "loss = -(J + 0.1 * entropy)\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_session(states, actions, rewards, t_max=1000):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    cumulative_rewards = get_cumulative_rewards(rewards)\n",
    "    update.run({\n",
    "        ph_states: states,\n",
    "        ph_actions: actions,\n",
    "        ph_cumulative_rewards: cumulative_rewards,\n",
    "    })\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer parameters\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 78.900\n",
      "mean reward: 191.240\n",
      "mean reward: 266.120\n",
      "mean reward: 224.150\n",
      "mean reward: 348.120\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [generate_session(env_monitor) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/openaigym.video.0.479.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open('rb') as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
